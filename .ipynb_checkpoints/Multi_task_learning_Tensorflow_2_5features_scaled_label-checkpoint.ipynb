{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "device_nb = 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#raw data\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "#subject_id = '38A'\n",
    "blood_analyte = 'CGM'\n",
    "\n",
    "xl = pd.ExcelFile('CGM_insulin_TG_data.xlsx')\n",
    "Meal_dfs = xl.parse(blood_analyte)\n",
    "sub_meal_idx = {}\n",
    "\n",
    "sub_LBM = {'38A': 39.47,\n",
    "           '38B': 62.78,\n",
    "           '38C': 53.5,\n",
    "           '38D': 37.472,\n",
    "           '38E': 45.52,\n",
    "           '38F': 43.38,\n",
    "           '38H': 47.39}\n",
    "\n",
    "X=[]\n",
    "Y1=[]\n",
    "Y2=[]\n",
    "Y3=[]\n",
    "\n",
    "for index, row in Meal_dfs.iterrows():\n",
    "    \n",
    "    #ignore missing meals\n",
    "    if not math.isnan(row.tolist()[6]):\n",
    "\n",
    "        #get each subject's meal index\n",
    "        for subject_id in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']:\n",
    "            if row['Patient_ID'] == subject_id:\n",
    "                if subject_id in sub_meal_idx:\n",
    "                    sub_meal_idx[subject_id] += [index]\n",
    "                else:\n",
    "                    sub_meal_idx[subject_id] = [index]\n",
    "           \n",
    "        \n",
    "    X += [np.float32(one_cgm) for one_cgm in row.tolist()[2:34]]\n",
    "\n",
    "    #carbs\n",
    "    if row['Meal_ID'][4]== '1':\n",
    "        Y1 += [np.float32(42.5/sub_LBM[row[0]])]\n",
    "        #Y1 += [np.float32(1)]\n",
    "    elif row['Meal_ID'][4]== '2':\n",
    "        Y1 += [np.float32(85/sub_LBM[row[0]])]\n",
    "        #Y1 += [np.float32(2)]\n",
    "    elif row['Meal_ID'][4]== '3':\n",
    "        Y1 += [np.float32(170/sub_LBM[row[0]])]\n",
    "        #Y1 += [np.float32(3)]\n",
    "\n",
    "    #protein\n",
    "    if row['Meal_ID'][1]== '1':\n",
    "        Y2 += [np.float32(15/sub_LBM[row[0]])]\n",
    "        #Y2 += [np.float32(1)]\n",
    "    elif row['Meal_ID'][1]== '2':\n",
    "        Y2 += [np.float32(30/sub_LBM[row[0]])]\n",
    "        #Y2 += [np.float32(2)]\n",
    "    elif row['Meal_ID'][1]== '3':\n",
    "        Y2 += [np.float32(60/sub_LBM[row[0]])]\n",
    "        #Y2 += [np.float32(3)]\n",
    "\n",
    "    #fat\n",
    "    if row['Meal_ID'][7]== '1':\n",
    "        Y3 += [np.float32(13/sub_LBM[row[0]])]    \n",
    "        #Y3 += [np.float32(1)]  \n",
    "    elif row['Meal_ID'][7]== '2':\n",
    "        Y3 += [np.float32(26/sub_LBM[row[0]])]   \n",
    "        #Y3 += [np.float32(2)]  \n",
    "    elif row['Meal_ID'][7]== '3':\n",
    "        Y3 += [np.float32(52/sub_LBM[row[0]])]  \n",
    "        #Y3 += [np.float32(3)]\n",
    "\n",
    "\n",
    "\n",
    "X_value = np.array(X).reshape(63,32)\n",
    "scaler = StandardScaler()\n",
    "X_value = scaler.fit_transform(X_value)\n",
    "Y1_value = np.array(Y1).reshape(-1,1)\n",
    "Y2_value = np.array(Y2).reshape(-1,1)\n",
    "Y3_value = np.array(Y3).reshape(-1,1)\n",
    "\n",
    "#Y1 = (Y1 - Y1.min())/(Y1.max()-Y1.min())\n",
    "#Y2 = (Y2 - Y2.min())/(Y2.max()-Y2.min())\n",
    "#Y3 = (Y3 - Y3.min())/(Y3.max()-Y3.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'38A': [0, 1, 2, 3, 4, 5, 6, 8],\n",
       " '38B': [10, 11, 12, 14, 15, 16, 17],\n",
       " '38C': [18, 20, 21, 22, 23, 24, 25, 26],\n",
       " '38D': [27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
       " '38E': [36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
       " '38F': [45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
       " '38H': [54, 55, 56, 58, 59, 61, 62]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_meal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#A: miss 8\\n#B: miss 1, 5\\n#C: miss 2\\n#H: miss 4, 7\\n#the others: meal 1 2 3 4 5 6 7 8 9 \\n\\nX_Gau = []\\n\\n\\nsub_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'H']\\n      \\nfor sub_l in sub_letters:\\n    \\n    if sub_l == 'A':\\n        miss_meal = [8]\\n    elif sub_l == 'B':\\n        miss_meal = [1, 5]\\n    elif sub_l == 'C':\\n        miss_meal = [2]\\n    elif sub_l == 'H':\\n        miss_meal = [4, 7]\\n    else:\\n        miss_meal = []\\n    \\n    with open('./Tianlong/Gauss Features 2018 1121/%s.txt'%sub_l, 'r') as f0:\\n        \\n        # each line is a meal\\n        for line_num in range(1,10):\\n            \\n            one_meal  = []\\n            \\n            if line_num in miss_meal:\\n                \\n                X_Gau.append(np.array([np.nan for nan_num in range(17)]))\\n                \\n            else:\\n                line = f0.readline()\\n\\n                for number in line.split(' '):\\n\\n                    one_meal += [float(number)]\\n\\n                one_meal = np.array(one_meal)\\n                X_Gau += [one_meal]\\n                \\nX_Gau = np.array(X_Gau)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "# 17 Gaussian AUC features (deprecated)\n",
    "########################################################\n",
    "\"\"\"\n",
    "#A: miss 8\n",
    "#B: miss 1, 5\n",
    "#C: miss 2\n",
    "#H: miss 4, 7\n",
    "#the others: meal 1 2 3 4 5 6 7 8 9 \n",
    "\n",
    "X_Gau = []\n",
    "\n",
    "\n",
    "sub_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'H']\n",
    "      \n",
    "for sub_l in sub_letters:\n",
    "    \n",
    "    if sub_l == 'A':\n",
    "        miss_meal = [8]\n",
    "    elif sub_l == 'B':\n",
    "        miss_meal = [1, 5]\n",
    "    elif sub_l == 'C':\n",
    "        miss_meal = [2]\n",
    "    elif sub_l == 'H':\n",
    "        miss_meal = [4, 7]\n",
    "    else:\n",
    "        miss_meal = []\n",
    "    \n",
    "    with open('./Tianlong/Gauss Features 2018 1121/%s.txt'%sub_l, 'r') as f0:\n",
    "        \n",
    "        # each line is a meal\n",
    "        for line_num in range(1,10):\n",
    "            \n",
    "            one_meal  = []\n",
    "            \n",
    "            if line_num in miss_meal:\n",
    "                \n",
    "                X_Gau.append(np.array([np.nan for nan_num in range(17)]))\n",
    "                \n",
    "            else:\n",
    "                line = f0.readline()\n",
    "\n",
    "                for number in line.split(' '):\n",
    "\n",
    "                    one_meal += [float(number)]\n",
    "\n",
    "                one_meal = np.array(one_meal)\n",
    "                X_Gau += [one_meal]\n",
    "                \n",
    "X_Gau = np.array(X_Gau)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 17 Gaussian AUC features\n",
    "########################################################\n",
    "\n",
    "\n",
    "data=pd.DataFrame(pd.read_excel('CGM_insulin_TG_data.xlsx'))\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    sample_am = data.shape[0]\n",
    "    protein = data.iloc[:, 1].values.reshape(-1, 1)\n",
    "    cho = data.iloc[:, 2].values.reshape(-1, 1)\n",
    "    fat = data.iloc[:, 3].values.reshape(-1, 1)\n",
    "    # pudding_A = data_38A.iloc[:,4].values.reshape(-1,1)\n",
    "    # water = data_38A.iloc[:,5].values.reshape(-1,1)\n",
    "    total_e = data.iloc[:, 6].values.reshape(-1, 1)\n",
    "    glucose = data.iloc[:, 2:35].values.reshape(sample_am, -1)\n",
    "    #glucose_last_column = glucose[:, -1]\n",
    "    #glucose_r = np.c_[glucose, glucose_last_column]\n",
    "    return protein, cho, fat, total_e, glucose\n",
    "\n",
    "def process_glucose(glucose):\n",
    "    a = np.zeros((1, 5))\n",
    "    for row in glucose:\n",
    "        # print(glucose_gaussian_feature_4(row).shape())\n",
    "        # print(glucose_gaussian_feature_8(row).shape())\n",
    "        # print(glucose_gaussian_feature_16(row).shape())\n",
    "        glucose_gaussian_feature = np.hstack([glucose_gaussian_feature_8(row)])\n",
    "        a = np.vstack([a, glucose_gaussian_feature])\n",
    "    # print(a.shape)\n",
    "    a = np.delete(a, 0, axis=0)\n",
    "    # print(a.shape)\n",
    "    return a\n",
    "\n",
    "def normal_function(mu, sigma, x):\n",
    "    normal_function_value = (1 / (sigma * np.sqrt(2 * np.pi))) * \\\n",
    "        np.exp(-((x - mu) ** 2) / (2 * (sigma ** 2)))\n",
    "    return normal_function_value\n",
    "\n",
    "def glucose_gaussian_feature_4(glucose_row):\n",
    "    gaussian_feature = []\n",
    "    normal_g_1 = normal_function(\n",
    "        0, 4/1.96, np.asarray([i for i in range(0, 5)])) * glucose_row[0:5]\n",
    "    normal_g_2 = normal_function(\n",
    "        4, 4/1.96, np.asarray([i for i in range(0, 9)])) * glucose_row[0:9]\n",
    "    normal_g_3 = normal_function(\n",
    "        8, 4/1.96, np.asarray([i for i in range(4, 13)])) * glucose_row[4:13]\n",
    "    normal_g_4 = normal_function(\n",
    "        12, 4/1.96, np.asarray([i for i in range(8, 17)])) * glucose_row[8:17]\n",
    "    normal_g_5 = normal_function(\n",
    "        16, 4/1.96, np.asarray([i for i in range(12, 21)])) * glucose_row[12:21]\n",
    "    normal_g_6 = normal_function(\n",
    "        20, 4/1.96, np.asarray([i for i in range(16, 25)])) * glucose_row[16:25]\n",
    "    normal_g_7 = normal_function(\n",
    "        24, 4/1.96, np.asarray([i for i in range(20, 29)])) * glucose_row[20:29]\n",
    "    normal_g_8 = normal_function(\n",
    "        28, 4/1.96, np.asarray([i for i in range(24, 33)])) * glucose_row[24:33]\n",
    "    normal_g_9 = normal_function(\n",
    "        32, 4/1.96, np.asarray([i for i in range(28, 33)])) * glucose_row[28:33]\n",
    "    for i in range(1, 10):\n",
    "        gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "    return gaussian_feature\n",
    "\n",
    "def glucose_gaussian_feature_8(glucose_row):\n",
    "    gaussian_feature = []\n",
    "    normal_g_1 = normal_function(\n",
    "        0, 8/1.96, np.asarray([i for i in range(0, 9)])) * glucose_row[0:9]\n",
    "    normal_g_2 = normal_function(\n",
    "        8, 8/1.96, np.asarray([i for i in range(0, 17)])) * glucose_row[0:17]\n",
    "    normal_g_3 = normal_function(\n",
    "        16, 8/1.96, np.asarray([i for i in range(8, 25)])) * glucose_row[8:25]\n",
    "    normal_g_4 = normal_function(\n",
    "        24, 8/1.96, np.asarray([i for i in range(16, 33)])) * glucose_row[16:33]\n",
    "    normal_g_5 = normal_function(\n",
    "        32, 8/1.96, np.asarray([i for i in range(24, 33)])) * glucose_row[24:33]\n",
    "    for i in range(1, 6):\n",
    "        gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "    return gaussian_feature\n",
    "\n",
    "def glucose_gaussian_feature_16(glucose_row):\n",
    "    gaussian_feature = []\n",
    "    normal_g_1 = normal_function(\n",
    "        0, 16/1.96, np.asarray([i for i in range(0, 17)])) * glucose_row[0:17]\n",
    "    normal_g_2 = normal_function(\n",
    "        16, 16/1.96, np.asarray([i for i in range(0, 33)])) * glucose_row[0:33]\n",
    "    normal_g_3 = normal_function(\n",
    "        32, 16/1.96, np.asarray([i for i in range(16, 33)])) * glucose_row[16:33]\n",
    "    for i in range(1, 4):\n",
    "        gaussian_feature.append(np.trapz(locals()[\"normal_g_\" + str(i)]))\n",
    "    return gaussian_feature\n",
    "\n",
    "\n",
    "p,c,f,e,r=process_data(data)\n",
    "newfeature=process_glucose(r)\n",
    "\n",
    "X_Gau = newfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014318031361695611"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_function(0,8/1.96,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38A\n",
      "c\n",
      "[2.1830227, 1.4140117, 1.349494, 1.073932, 3.4372675, 1.55214, 1.1032292, 1.5528644] [1.0767672, 2.1535344, 4.307069, 1.0767672, 4.307069, 2.1535344, 2.1535344, 2.1535344]\n",
      "p\n",
      "[0.97238445, 0.6011135, 0.5699649, 0.43692636, 1.5779214, 0.6678004, 0.45107076, 0.6681502] [0.38003546, 0.7600709, 1.5201418, 0.7600709, 0.7600709, 1.5201418, 0.38003546, 0.7600709]\n",
      "f\n",
      "[1.1713455, 0.51220095, 0.45690057, 0.22070745, 2.2463999, 0.6305951, 0.245819, 0.63121617] [0.32936406, 0.6587281, 1.3174562, 0.6587281, 0.6587281, 0.6587281, 0.6587281, 0.32936406]\n",
      "38B\n",
      "c\n",
      "[2.3048608, 1.919074, 1.6217775, 2.251118, 0.6735688, 0.6735688, 1.3480463] [1.3539344, 2.7078688, 0.6769672, 1.3539344, 1.3539344, 1.3539344, 1.3539344]\n",
      "p\n",
      "[0.9690048, 0.9855842, 0.6748988, 0.9458654, 0.26664132, 0.26664132, 0.557042] [0.4778592, 0.9557184, 0.4778592, 0.9557184, 0.2389296, 0.4778592, 0.4778592]\n",
      "f\n",
      "[1.0789785, 0.4996305, 0.53418905, 1.0361161, -0.22204997, -0.22204997, 0.31587607] [0.41414464, 0.8282893, 0.41414464, 0.41414464, 0.41414464, 0.8282893, 0.20707232]\n",
      "38C\n",
      "c\n",
      "[2.2915056, 2.8088367, 1.9392694, 2.2361593, 1.85175, 1.9176183, 2.5478919, 1.6053363] [0.7943925, 3.17757, 0.7943925, 3.17757, 1.588785, 1.588785, 1.588785, 1.588785]\n",
      "p\n",
      "[0.94491243, 1.1715724, 0.79058605, 0.92066336, 0.75224084, 0.7810999, 1.0572438, 0.6442788] [0.28037384, 1.1214954, 0.5607477, 0.5607477, 1.1214954, 0.28037384, 0.5607477, 0.5607477]\n",
      "f\n",
      "[1.0893593, 1.5185533, 0.7971332, 1.0434422, 0.72452426, 0.77917063, 1.3020654, 0.5200918] [0.24299066, 0.97196263, 0.48598132, 0.48598132, 0.48598132, 0.48598132, 0.97196263, 0.24299066]\n",
      "38D\n",
      "c\n",
      "[2.1535969, 0.89330125, 0.58392245, 1.2338307, 0.58392245, 1.9514105, 1.8484995, 2.3762472, 1.4924536] [1.1341802, 2.2683604, 4.5367208, 1.1341802, 4.5367208, 2.2683604, 2.2683604, 2.2683604, 2.2683604]\n",
      "p\n",
      "[0.8768022, 0.33446234, 0.20132813, 0.4810015, 0.20132813, 0.7897959, 0.74551046, 0.97261477, 0.5922941] [0.4002989, 0.8005978, 1.6011956, 0.8005978, 0.8005978, 1.6011956, 0.4002989, 0.8005978, 0.8005978]\n",
      "f\n",
      "[1.036684, -0.01113376, -0.26835328, 0.27198452, -0.26835328, 0.86858505, 0.78302413, 1.221797, 0.4870053] [0.3469257, 0.6938514, 1.3877028, 0.6938514, 0.6938514, 0.6938514, 0.6938514, 1.3877028, 0.3469257]\n",
      "38E\n",
      "c\n",
      "[1.7542983, 1.5453234, 4.2843924, 1.5619059, 0.9246479, 1.8884735, 1.007073, 2.6775608, 1.3283103] [0.93365556, 1.8673111, 3.7346222, 0.93365556, 3.7346222, 1.8673111, 1.8673111, 1.8673111, 1.8673111]\n",
      "p\n",
      "[0.74374545, 0.6580492, 1.781283, 0.66484934, 0.40352345, 0.7987678, 0.43732426, 1.1223558, 0.56905675] [0.32952547, 0.65905094, 1.3181019, 0.65905094, 0.65905094, 1.3181019, 0.32952547, 0.65905094, 0.65905094]\n",
      "f\n",
      "[0.68628013, 0.51976204, 2.7023418, 0.53297544, 0.025187522, 0.79319537, 0.090866566, 1.421966, 0.34683886] [0.28558874, 0.5711775, 1.142355, 0.5711775, 0.5711775, 0.5711775, 0.5711775, 1.142355, 0.28558874]\n",
      "38F\n",
      "c\n",
      "[1.9206326, 3.215832, 0.77462554, 1.9948647, 0.48800987, 1.3387029, 2.9670837, 0.48800987, 2.7999768] [0.97971416, 1.9594283, 3.9188566, 0.97971416, 3.9188566, 1.9594283, 1.9594283, 1.9594283, 1.9594283]\n",
      "p\n",
      "[0.75960356, 1.2899563, 0.29034153, 0.7899998, 0.17297937, 0.5213175, 1.1880999, 0.17297937, 1.1196737] [0.34578148, 0.69156295, 1.3831259, 0.69156295, 0.69156295, 1.3831259, 0.34578148, 0.69156295, 0.69156295]\n",
      "f\n",
      "[0.78114855, 1.814715, -0.13336292, 0.84038556, -0.36208168, 0.3167697, 1.6162145, -0.36208168, 1.4828635] [0.29967728, 0.59935457, 1.1987091, 0.59935457, 0.59935457, 0.59935457, 0.59935457, 1.1987091, 0.29967728]\n",
      "38H\n",
      "c\n",
      "[1.0722454, 1.0315387, 0.6513019, 1.4350326, 1.2982659, 1.2484531, 0.9142882] [0.8968137, 1.7936274, 3.5872548, 3.5872548, 1.7936274, 1.7936274, 1.7936274]\n",
      "p\n",
      "[0.4370417, 0.41827762, 0.24300441, 0.6042713, 0.5412277, 0.5182661, 0.36423007] [0.31652248, 0.63304496, 1.2660899, 0.63304496, 1.2660899, 0.63304496, 0.63304496]\n",
      "f\n",
      "[0.14331502, 0.1096099, -0.20522594, 0.44370252, 0.33045983, 0.28921494, 0.012526557] [0.27431947, 0.54863894, 1.0972779, 0.54863894, 0.54863894, 1.0972779, 0.27431947]\n",
      "2 0.0005 0.001 0.001\n",
      "{'38A': {'c': (0.45656303, 0.25546806461299987), 'p': (-0.14160548, 0.738017731808098), 'f': (-0.20540634, 0.6255590659886224)}, '38B': {'c': (0.18362075, 0.6935041842248664), 'p': (0.7401876, 0.05712367013884895), 'f': (-0.29422578, 0.5218433498244137)}, '38C': {'c': (0.48161855, 0.226891025230167), 'p': (0.25089023, 0.5489486904359095), 'f': (0.77726704, 0.023215352092074867)}, '38D': {'c': (-0.67431736, 0.04636358055982795), 'p': (-0.31387377, 0.4107764717178666), 'f': (-0.10890635, 0.7803189639776533)}, '38E': {'c': (0.3668413, 0.3314918991561665), 'p': (0.64029145, 0.06322341148698235), 'f': (0.7787454, 0.013407566212592922)}, '38F': {'c': (-0.5294774, 0.14265357210536705), 'p': (-0.45137233, 0.22263131582796622), 'f': (-0.61571723, 0.07751496529713403)}, '38H': {'c': (-0.09989174, 0.8312633247539302), 'p': (-0.2528541, 0.5843312737180513), 'f': (-0.18870234, 0.6853219767399527)}}\n",
      "266.27606\n",
      "377.70386\n",
      "146.48335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/g/guangzhou92/enter/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38A\n",
      "c\n",
      "[1.3626853, 1.2350768, 2.479689, 1.5900135, 1.9168916, 2.0701053, 1.5506543, 1.4488695] [1.0767672, 2.1535344, 4.307069, 1.0767672, 4.307069, 2.1535344, 2.1535344, 2.1535344]\n",
      "p\n",
      "[0.3718983, 0.30727747, 0.93754745, 0.48701698, 0.65254754, 0.73013484, 0.46708554, 0.41554177] [0.38003546, 0.7600709, 1.5201418, 0.7600709, 0.7600709, 1.5201418, 0.38003546, 0.7600709]\n",
      "f\n",
      "[0.50558525, 0.46708822, 0.84256333, 0.5741657, 0.67277837, 0.719, 0.56229174, 0.5315853] [0.32936406, 0.6587281, 1.3174562, 0.6587281, 0.6587281, 0.6587281, 0.6587281, 0.32936406]\n",
      "38B\n",
      "c\n",
      "[1.9222207, 1.902602, 1.6715813, 1.9733627, 1.528989, 1.758887, 1.7309825] [1.3539344, 2.7078688, 0.6769672, 1.3539344, 1.3539344, 1.3539344, 1.3539344]\n",
      "p\n",
      "[0.61654264, 0.60701925, 0.4948768, 0.64136815, 0.42565936, 0.5372569, 1.7147024] [0.4778592, 0.9557184, 0.4778592, 0.9557184, 0.2389296, 0.4778592, 0.4778592]\n",
      "f\n",
      "[0.69383603, 0.68773454, 0.61588657, 0.70974135, 0.57153994, 0.64303887, 1.3600852] [0.41414464, 0.8282893, 0.41414464, 0.41414464, 0.41414464, 0.8282893, 0.20707232]\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#training + parameter search\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Y1=carbs     Y2=protein      Y3=fat\n",
    "\n",
    "\n",
    "#options: X_value     X_Gau\n",
    "Input_X = X_Gau\n",
    "\n",
    "#loss => all default huber\n",
    "\n",
    "#choose between L1SO vs. L1MO\n",
    "scenario = 'L1SO'\n",
    "\n",
    "L1MO_test_subs = ['38A', '38B', '38C', '38D', '38E', '38F', '38H']\n",
    "\n",
    "neuron_shared_ls = [2, 3, 4, 5]\n",
    "\n",
    "learning_rate_1_ls = [0.0005, 0.001, 0.005]\n",
    "\n",
    "learning_rate_2_ls = [0.001, 0.005]\n",
    "\n",
    "learning_rate_3_ls = [0.001, 0.005]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_epochs = 111\n",
    "\n",
    "\n",
    "#____________________________________________\n",
    "#if not for grid search, can be commented out\n",
    "\n",
    "for neurons_shared in neuron_shared_ls:\n",
    "    for learning_rate_1 in learning_rate_1_ls:\n",
    "        for learning_rate_2 in learning_rate_2_ls:\n",
    "            for learning_rate_3 in learning_rate_3_ls:\n",
    "\n",
    "                '''\n",
    "                neurons_shared = 3\n",
    "                learning_rate_1 = 0.001\n",
    "                learning_rate_2 = 0.005\n",
    "                learning_rate_3 = 0.005\n",
    "                '''\n",
    "\n",
    "\n",
    "                # ======================\n",
    "                # Define the Graph\n",
    "                # ======================\n",
    "\n",
    "                # Define the Placeholders\n",
    "                X = tf.placeholder(\"float\", [None, Input_X.shape[1]], name=\"X\")\n",
    "                Y1 = tf.placeholder(\"float\", [None, 1], name=\"Y1\")\n",
    "                Y2 = tf.placeholder(\"float\", [None, 1], name=\"Y2\")\n",
    "                Y3 = tf.placeholder(\"float\", [None, 1], name=\"Y3\")\n",
    "\n",
    "                # Define the weights for the layers\n",
    "                initial_shared_layer_weights = np.random.rand(Input_X.shape[1],neurons_shared)\n",
    "                initial_Y1_layer_weights = np.random.rand(neurons_shared,1)\n",
    "                initial_Y2_layer_weights = np.random.rand(neurons_shared,1)\n",
    "                initial_Y3_layer_weights = np.random.rand(neurons_shared,1)\n",
    "\n",
    "                shared_layer_weights = tf.Variable(initial_shared_layer_weights, name=\"share_W\", dtype=\"float32\")\n",
    "                Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name=\"share_Y1\", dtype=\"float32\")\n",
    "                Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name=\"share_Y2\", dtype=\"float32\")\n",
    "                Y3_layer_weights = tf.Variable(initial_Y3_layer_weights, name=\"share_Y3\", dtype=\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "                #Define the biases for the layers\n",
    "                initial_shared_layer_bias = np.random.rand(neurons_shared,)\n",
    "                initial_Y1_layer_bias = np.random.rand(1)\n",
    "                initial_Y2_layer_bias = np.random.rand(1)\n",
    "                initial_Y3_layer_bias = np.random.rand(1)\n",
    "\n",
    "                shared_layer_bias = tf.Variable(initial_shared_layer_bias, name=\"share_B\", dtype=\"float32\")\n",
    "                Y1_layer_bias = tf.Variable(initial_Y1_layer_bias, name=\"share_Y1_B\", dtype=\"float32\")\n",
    "                Y2_layer_bias = tf.Variable(initial_Y2_layer_bias, name=\"share_Y2_B\", dtype=\"float32\")\n",
    "                Y3_layer_bias = tf.Variable(initial_Y3_layer_bias, name=\"share_Y3_B\", dtype=\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "                # Construct the Layers with RELU Activations\n",
    "                shared_layer = tf.nn.relu(tf.add(tf.matmul(X,shared_layer_weights), shared_layer_bias))\n",
    "                #Y1_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y1_layer_weights), Y1_layer_bias))\n",
    "                #Y2_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y2_layer_weights), Y2_layer_bias))\n",
    "                #Y3_layer = tf.nn.relu(tf.add(tf.matmul(shared_layer,Y3_layer_weights), Y3_layer_bias))\n",
    "                Y1_layer = tf.add(tf.matmul(shared_layer,Y1_layer_weights), Y1_layer_bias)\n",
    "                Y2_layer = tf.add(tf.matmul(shared_layer,Y2_layer_weights), Y2_layer_bias)\n",
    "                Y3_layer = tf.add(tf.matmul(shared_layer,Y3_layer_weights), Y3_layer_bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Calculate Loss\n",
    "                #Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)\n",
    "                #Y1_Loss = tf.norm(Y1-Y1_layer, ord= 1)\n",
    "                #Y1_Loss = tf.losses.mean_squared_error(Y1, Y1_layer)\n",
    "                Y1_Loss = tf.losses.huber_loss(Y1, Y1_layer)\n",
    "                #Y1_Loss = tf.keras.losses.logcosh(Y1, Y1_layer)\n",
    "\n",
    "                #Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)\n",
    "                #Y2_Loss = tf.norm(Y2-Y2_layer, ord= 1)\n",
    "                #Y2_Loss = tf.losses.mean_squared_error(Y2, Y2_layer)\n",
    "                Y2_Loss = tf.losses.huber_loss(Y2, Y2_layer)\n",
    "\n",
    "                #Y3_Loss = tf.nn.l2_loss(Y3-Y3_layer)\n",
    "                #Y3_Loss = tf.norm(Y3-Y3_layer, ord= 1)\n",
    "                #Y3_Loss = tf.losses.mean_squared_error(Y3, Y3_layer)\n",
    "                Y3_Loss = tf.losses.huber_loss(Y3, Y3_layer)\n",
    "\n",
    "                Joint_Loss = Y1_Loss + Y2_Loss + Y3_Loss\n",
    "\n",
    "                # optimisers\n",
    "                Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)\n",
    "                Y1_op = tf.train.AdamOptimizer(learning_rate=learning_rate_1, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y1_Loss)\n",
    "                Y2_op = tf.train.AdamOptimizer(learning_rate=learning_rate_2, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y2_Loss)\n",
    "                Y3_op = tf.train.AdamOptimizer(learning_rate=learning_rate_3, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(Y3_Loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #__________________________________________________________________________________________________________\n",
    "                # Joint Training\n",
    "                # Calculation (Session) Code\n",
    "                # ==========================\n",
    "\n",
    "                # open the session\n",
    "                session = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ############### L1SO ####################\n",
    "                if scenario == 'L1SO':\n",
    "\n",
    "                    sub_to_correlation = {}\n",
    "\n",
    "\n",
    "                    for test_sub in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']: \n",
    "\n",
    "\n",
    "                        if test_sub not in sub_to_correlation:\n",
    "                            sub_to_correlation[test_sub] = {}\n",
    "\n",
    "\n",
    "                        Y1_loss_list = []\n",
    "                        Y2_loss_list = []\n",
    "                        Y3_loss_list = []\n",
    "                        Joint_loss_list = []\n",
    "\n",
    "\n",
    "                        session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "                        #######  training phase  ###########\n",
    "                        for i in range(1, N_epochs):\n",
    "\n",
    "\n",
    "                            train_meal_idx = []\n",
    "                            for one_sub in sub_meal_idx:\n",
    "                                if one_sub != test_sub:\n",
    "                                    train_meal_idx += sub_meal_idx[one_sub]\n",
    "\n",
    "                            random.shuffle(train_meal_idx)\n",
    "\n",
    "\n",
    "                            #each meal will be one batch, and only update one out of three loss\n",
    "                            for meal_idx in train_meal_idx:\n",
    "                                rand_nb = np.random.rand()\n",
    "                                if rand_nb < 0.25:\n",
    "                                    _, Y1_loss = session.run([Y1_op, Y1_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "\n",
    "                                    Y1_loss_list += [float(str(Y1_loss))]\n",
    "\n",
    "                                elif 0.25 <= rand_nb < 0.5:\n",
    "                                    _, Y2_loss = session.run([Y2_op, Y2_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "\n",
    "                                    Y2_loss_list += [float(str(Y2_loss))]\n",
    "\n",
    "                                elif 0.5 <= rand_nb < 0.75:\n",
    "                                    _, Y3_loss = session.run([Y3_op, Y3_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "\n",
    "                                    Y3_loss_list += [float(str(Y3_loss))]\n",
    "\n",
    "                                else:\n",
    "                                    _, Joint_loss = session.run([Optimiser, Joint_Loss],\n",
    "                                                    {\n",
    "                                                      X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                      Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                      Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                      })\n",
    "                                    Joint_loss_list += [float(str(Joint_loss))]\n",
    "\n",
    "\n",
    "\n",
    "                            if i % 2000 == 0:\n",
    "                                print('y1 loss: ', Y1_loss, end=' |')\n",
    "                                print('y2 loss: ', Y2_loss, end=' |')\n",
    "                                print('y3 loss: ', Y3_loss, end=' |')\n",
    "                                print('joint loss: ', Joint_loss)\n",
    "\n",
    "\n",
    "\n",
    "                        #######  testing phase  ###########\n",
    "                        all_pred = []\n",
    "                        all_true = []\n",
    "\n",
    "                        for test_meal in sub_meal_idx[test_sub]:\n",
    "                            Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                                                  X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                                                  Y1: Y1_value[test_meal].reshape(-1,1),\n",
    "                                                  Y2: Y2_value[test_meal].reshape(-1,1),\n",
    "                                                  Y3: Y3_value[test_meal].reshape(-1,1)\n",
    "                                                  })\n",
    "                            all_pred += [[Y1_pred[0][0], Y2_pred[0][0], Y3_pred[0][0]]]\n",
    "                            all_true += [[Y1_value[test_meal][0], Y2_value[test_meal][0], Y3_value[test_meal][0]]]\n",
    "\n",
    "\n",
    "\n",
    "                        #######  plotting phase  ###########\n",
    "                        corr_cx = []\n",
    "                        corr_cy = []\n",
    "                        corr_px = []\n",
    "                        corr_py = []\n",
    "                        corr_fx = []\n",
    "                        corr_fy = []\n",
    "\n",
    "\n",
    "                        for meal_num in range(len(all_true)):\n",
    "                            corr_cx.append(all_pred[meal_num][0])\n",
    "                            corr_cy.append(all_true[meal_num][0])\n",
    "\n",
    "                            corr_px.append(all_pred[meal_num][1])\n",
    "                            corr_py.append(all_true[meal_num][1])\n",
    "\n",
    "                            corr_fx.append(all_pred[meal_num][2])\n",
    "                            corr_fy.append(all_true[meal_num][2])    \n",
    "\n",
    "                        sub_to_correlation[test_sub] = {'c':scipy.stats.pearsonr(corr_cx,corr_cy),\n",
    "                                                        'p':scipy.stats.pearsonr(corr_px,corr_py),\n",
    "                                                        'f':scipy.stats.pearsonr(corr_fx,corr_fy)}\n",
    "\n",
    "                        \n",
    "                       \n",
    "                        \n",
    "                        '''\n",
    "                        plt.subplot(411)\n",
    "                        plt.plot(Y1_loss_list)\n",
    "                        plt.subplot(412)\n",
    "                        plt.plot(Y2_loss_list)\n",
    "                        plt.subplot(413)\n",
    "                        plt.plot(Y3_loss_list)\n",
    "                        plt.subplot(414)\n",
    "                        plt.plot(Joint_loss_list)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "                        \n",
    "                        print(test_sub)\n",
    "                        print('c')\n",
    "                        print(corr_cx, corr_cy)\n",
    "                        print('p')\n",
    "                        print(corr_px, corr_py)\n",
    "                        print('f')\n",
    "                        print(corr_fx, corr_fy) \n",
    "                        \n",
    "                        \n",
    "                    print(neurons_shared, learning_rate_1, learning_rate_2, learning_rate_3) \n",
    "                    print(sub_to_correlation)\n",
    "                        \n",
    "\n",
    "                    print(max(Y1_loss_list))\n",
    "                    print(max(Y2_loss_list))  \n",
    "                    print(max(Y3_loss_list))\n",
    "\n",
    "\n",
    "\n",
    "                ################# L1MO ####################       \n",
    "                elif scenario == 'L1MO':\n",
    "\n",
    "                    sub_to_correlation = {}\n",
    "\n",
    "                    #for test_sub in ['38A', '38B', '38C', '38D', '38E', '38F', '38H']: \n",
    "                    #for test_sub in ['38B']: \n",
    "                    for test_sub in L1MO_test_subs:\n",
    "\n",
    "                        if test_sub not in sub_to_correlation:\n",
    "                             sub_to_correlation[test_sub] = {}\n",
    "\n",
    "\n",
    "                        Y1_loss_list = []\n",
    "                        Y2_loss_list = []\n",
    "                        Y3_loss_list = []\n",
    "                        Joint_loss_list = []\n",
    "\n",
    "                        session.run(tf.global_variables_initializer())\n",
    "\n",
    "                        all_meal_pred = []\n",
    "                        all_meal_true = []\n",
    "\n",
    "                        for test_meal in sub_meal_idx[test_sub]:\n",
    "\n",
    "                            #create training indexes\n",
    "                            train_meal_idxes = []\n",
    "                            for meal_index in sub_meal_idx[test_sub]:\n",
    "                                if meal_index != test_meal:\n",
    "                                    train_meal_idxes.append(meal_index) \n",
    "\n",
    "                            print(test_sub, test_meal, train_meal_idxes)\n",
    "                            random.shuffle(train_meal_idxes)\n",
    "\n",
    "\n",
    "                            #######  training phase  ###########\n",
    "                            for i in range(1,N_epochs):\n",
    "                                for meal_idx in train_meal_idxes:\n",
    "\n",
    "                                    rand_nb = np.random.rand()\n",
    "                                    if rand_nb < 0.333:\n",
    "                                        _, Y1_loss = session.run([Y1_op, Y1_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "\n",
    "                                        Y1_loss_list += [float(str(Y1_loss))]\n",
    "\n",
    "                                    elif 0.333 <= rand_nb < 0.666:\n",
    "                                        _, Y2_loss = session.run([Y2_op, Y2_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "\n",
    "                                        Y2_loss_list += [float(str(Y2_loss))]\n",
    "\n",
    "                                    elif 0.666 <= rand_nb < 0.999:\n",
    "                                        _, Y3_loss = session.run([Y3_op, Y3_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "\n",
    "                                        Y3_loss_list += [float(str(Y3_loss))]\n",
    "\n",
    "                                    '''    \n",
    "                                    else:\n",
    "                                        _, Joint_loss = session.run([Optimiser, Joint_Loss],\n",
    "                                                        {\n",
    "                                                          X: Input_X[meal_idx, :].reshape(-1,Input_X.shape[1]),\n",
    "                                                          Y1: Y1_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y2: Y2_value[meal_idx, :].reshape(-1,1),\n",
    "                                                          Y3: Y3_value[meal_idx, :].reshape(-1,1)\n",
    "                                                          })\n",
    "                                        Joint_loss_list += [float(str(Joint_loss))]\n",
    "                                    '''\n",
    "\n",
    "\n",
    "                            if i % 50 == 0:\n",
    "                                print('y1 loss: ', Y1_loss, end=' |')\n",
    "                                print('y2 loss: ', Y2_loss, end=' |')\n",
    "                                print('y3 loss: ', Y3_loss, end=' |')\n",
    "                                print('joint loss: ', Joint_loss)\n",
    "\n",
    "\n",
    "\n",
    "                            #######  testing phase  ###########\n",
    "                            Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                                                  X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                                                  Y1: Y1_value[test_meal].reshape(-1,1),\n",
    "                                                  Y2: Y2_value[test_meal].reshape(-1,1),\n",
    "                                                  Y3: Y3_value[test_meal].reshape(-1,1)\n",
    "                                                  })\n",
    "\n",
    "                            all_meal_pred += [[Y1_pred[0][0], Y2_pred[0][0], Y3_pred[0][0]]]\n",
    "                            all_meal_true += [[Y1_value[test_meal][0], Y2_value[test_meal][0], Y3_value[test_meal][0]]]\n",
    "\n",
    "\n",
    "                        #######  plotting phase  ###########\n",
    "                        corr_cx = []\n",
    "                        corr_cy = []\n",
    "                        corr_px = []\n",
    "                        corr_py = []\n",
    "                        corr_fx = []\n",
    "                        corr_fy = []\n",
    "\n",
    "\n",
    "                        for meal_num in range(len(all_meal_true)):\n",
    "                            corr_cx.append(all_meal_pred[meal_num][0])\n",
    "                            corr_cy.append(all_meal_true[meal_num][0])\n",
    "\n",
    "                            corr_px.append(all_meal_pred[meal_num][1])\n",
    "                            corr_py.append(all_meal_true[meal_num][1])\n",
    "\n",
    "                            corr_fx.append(all_meal_pred[meal_num][2])\n",
    "                            corr_fy.append(all_meal_true[meal_num][2])    \n",
    "\n",
    "                        sub_to_correlation[test_sub] = {'c':scipy.stats.pearsonr(corr_cx,corr_cy),\n",
    "                                                        'p':scipy.stats.pearsonr(corr_px,corr_py),\n",
    "                                                        'f':scipy.stats.pearsonr(corr_fx,corr_fy)}\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        '''\n",
    "                        plt.subplot(411)\n",
    "                        plt.plot(Y1_loss_list)\n",
    "                        plt.subplot(412)\n",
    "                        plt.plot(Y2_loss_list)\n",
    "                        plt.subplot(413)\n",
    "                        plt.plot(Y3_loss_list)\n",
    "                        plt.subplot(414)\n",
    "                        plt.plot(Joint_loss_list)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "                        \n",
    "                    print(neurons_shared, learning_rate_1, learning_rate_2, learning_rate_3)    \n",
    "                    print(sub_to_correlation)\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "\n",
    "\n",
    "                    #print(max(Y1_loss_list))\n",
    "                    #print(max(Y2_loss_list))  \n",
    "                    #print(max(Y3_loss_list))\n",
    "\n",
    "\n",
    "session.close()\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_to_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sub_to_correlation['38A']['c'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meal_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub_to_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.run(Joint_Loss, {\n",
    "                          X: Input_X[sub_meal_idx[test_sub][2]].reshape(-1,Input_X.shape[1]),\n",
    "                          Y1: Y1_value[sub_meal_idx[test_sub][2]].reshape(-1,1),\n",
    "                          Y2: Y2_value[sub_meal_idx[test_sub][2]].reshape(-1,1),\n",
    "                          Y3: Y3_value[sub_meal_idx[test_sub][2]].reshape(-1,1)\n",
    "                          }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = []\n",
    "all_true = []\n",
    "\n",
    "\n",
    "for test_meal in sub_meal_idx[test_sub]:\n",
    "    Y1_pred, Y2_pred, Y3_pred = session.run([Y1_layer,Y2_layer,Y3_layer], {\n",
    "                          X: Input_X[test_meal].reshape(-1,Input_X.shape[1]),\n",
    "                          Y1: Y1_value[test_meal].reshape(-1,1),\n",
    "                          Y2: Y2_value[test_meal].reshape(-1,1),\n",
    "                          Y3: Y3_value[test_meal].reshape(-1,1)\n",
    "                          })\n",
    "    all_pred += [[Y1_pred[0][0], Y2_pred[0][0], Y3_pred[0][0]]]\n",
    "    all_true += [[Y1_value[test_meal][0], Y2_value[test_meal][0], Y3_value[test_meal][0]]]\n",
    "    print(Y1_pred, Y2_pred, Y3_pred)\n",
    "    print(Y1_value[test_meal], Y2_value[test_meal], Y3_value[test_meal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cx = []\n",
    "corr_cy = []\n",
    "corr_px = []\n",
    "corr_py = []\n",
    "corr_fx = []\n",
    "corr_fy = []\n",
    "\n",
    "\n",
    "for meal_num in range(len(all_true)):\n",
    "    corr_cx.append(all_pred[meal_num][0])\n",
    "    corr_cy.append(all_true[meal_num][0])\n",
    "    \n",
    "    corr_px.append(all_pred[meal_num][1])\n",
    "    corr_py.append(all_true[meal_num][1])\n",
    "    \n",
    "    corr_fx.append(all_pred[meal_num][2])\n",
    "    corr_fy.append(all_true[meal_num][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(corr_cx,corr_cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(corr_px,corr_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(corr_fx,corr_fy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
